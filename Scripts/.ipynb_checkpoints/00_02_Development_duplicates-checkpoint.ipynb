{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development\n",
    "\n",
    "This script is for development of other functions - just for simplicity of execution etc. Later the code should be moved to *football_functions*, and then deleted from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 0: Initial packages and definitions\n",
    "\n",
    "Just a block to define some stuff that we will probably never be changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base packages for running the script\n",
    "import sys, datetime\n",
    "\n",
    "# Set the path and proxy in accordance with our OS\n",
    "if sys.platform == 'linux':\n",
    "    HOME_PATH = '/home/andreas/Desktop/Projects/Football/'\n",
    "    proxy_settings = None\n",
    "else:\n",
    "    HOME_PATH = 'c:/Users/amathewl/Desktop/3_Personal_projects/football/'\n",
    "    proxy_settings = None\n",
    "    \n",
    "# Relative paths\n",
    "data_loc = HOME_PATH + 'Data_work/Data/'\n",
    "html_loc = data_loc + '01_HTML/'\n",
    "organ_loc = data_loc + '00_Organisation/'\n",
    "story_loc = data_loc + '02_Stories/'\n",
    "\n",
    "# Get today's date for various functions\n",
    "date_today = datetime.datetime.today().strftime('%Y_%m_%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a logger for development\n",
    "from football_functions.generic import default_logger\n",
    "\n",
    "dev_logger = default_logger.get_logger(data_loc, date_today, 'development')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 1: Function to clean up downloaded data\n",
    "\n",
    "WILL TEST USING BBC DATA FOR A FEW DAYS\n",
    "\n",
    "The idea will be to build a post-processing clean up function that will let us delete anything that is duplicated.\n",
    "\n",
    "Initially this will serve to save space by deleting stuff that is already duplicated, but in future it should serve as a way to delete pulled HTML before ever even looking at it.\n",
    "\n",
    "Because of the way things work - this function should be executed after EVERY SINGLE data capture stage - but also keep a version for a general clean up.\n",
    "\n",
    "1. First delete sublink HTML that is identical to something previously pulled. This is the first step because an identical URL can return different results depending on the date (e.g. football/teampages/Arsenal) - but if the HTML is the same, then all of the stories on that page are the same (as, by definition, the suburl is where we look for headlines). Once this is done, none of the contained headlines will be searched, saving a lot.\n",
    "\n",
    "2. Secondly, delete stories that are identical. This is self explanatory - and will save time, but especially HTML. There could be a slight aspect where stories could actually be updated at a later date - will ignore this case for now. This step involves looking at the story pickles (as that is where we directly saved to from sublinks), and then deleting based on info - and also further deleting HTML.\n",
    "\n",
    "Two things being identical is determined by:\n",
    "\n",
    "* Checking whether or not the end file name is the same, as there should be nothing making us save the same article under different file names, beyond the location.\n",
    "\n",
    "* Checking whether or not the actual HTML/content is identical - this is important as having the same file name does not guarantee anything. \n",
    "\n",
    "When we delete files, we will always keep the oldest one. \n",
    "\n",
    "We will also keep a record of those files that have caused deletions in the past (for both stages), and do an initial check on those ones (this prevents us needing to look through all of our dates for a given file type to delete) - will keep these as a CSV where the columns are path and name\n",
    "\n",
    "NEED TO BE CAREFUL AND UPDATE LOG IF WE HAVE AN OLDER FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, pandas as pd\n",
    "\n",
    "# Log file names where we will save the full path to previously deleting files - keep this as FILE_NAME / PATH / TYPE OF DELETION\n",
    "log_file = os.path.join(organ_loc, 'file_deletion.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 2: Function definitions\n",
    "\n",
    "Will define a function to check whether the content found at two paths are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The checking process isn't good - I think the only way to do it properly will be to actually retrieve the info first and then process it.\n",
    "\n",
    "Considering that the processing of screening the HTMLs is not that heavy - could easily just repeat the process for anything that appears as a duplicate. \n",
    "In the long run, implementing everything in a chain, wouldn't even have an issue, as could just check it and then keep it if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import football_functions.source_specific.bbc.process_html as bbc # then bbc.process_html.XXX\n",
    "import football_functions.source_specific.dailymail.process_html as dailymail\n",
    "import football_functions.source_specific.mirror.process_html as mirror\n",
    "import football_functions.source_specific.guardian.process_html as guardian\n",
    "import football_functions.source_specific.skysports.process_html as skysports\n",
    "import football_functions.source_specific.telegraph.process_html as telegraph\n",
    "\n",
    "from football_functions.source_specific.bbc import process_html as bbc\n",
    "from football_functions.source_specific.dailymail import process_html as dailymail\n",
    "from football_functions.source_specific.guardian import process_html as guardian\n",
    "from football_functions.source_specific.mirror import process_html as mirror\n",
    "from football_functions.source_specific.skysports import process_html as skysports\n",
    "from football_functions.source_specific.telegraph import process_html as telegraph\n",
    "\n",
    "def check_html(check_1, check_2, domain, is_sublinks, logger):\n",
    "    '''\n",
    "    Function to check if 2 HTMLs are the same by doing some set operations\n",
    "    '''\n",
    "    \n",
    "    # If looking at sublinks then pull the links\n",
    "    if is_sublinks:\n",
    "        # Now do source specific stuff\n",
    "        if domain == 'bbc':\n",
    "            modifier = 'football_teams' in check_1\n",
    "            articles_info_1 = bbc.extract_headlines(check_1, modifier, logger)\n",
    "            articles_info_2 = bbc.extract_headlines(check_2, modifier, logger)\n",
    "\n",
    "        elif domain == 'dailymail':\n",
    "            modifier = 'football_index' not in check_1\n",
    "            articles_info_1 = dailymail.extract_headlines(check_1, modifier, logger)\n",
    "            articles_info_2 = dailymail.extract_headlines(check_2, modifier, logger)\n",
    "\n",
    "        elif domain == 'theguardian':\n",
    "            articles_info_1 = guardian.extract_headlines(check_1, logger)\n",
    "            articles_info_2 = guardian.extract_headlines(check_2, logger)\n",
    "\n",
    "        elif domain == 'mirror':\n",
    "            articles_info_1 = mirror.extract_headlines(check_1, logger)\n",
    "            articles_info_2 = mirror.extract_headlines(check_2, logger)\n",
    "\n",
    "        elif domain == 'skysports':\n",
    "            modifier = 'regional' in check_1\n",
    "            articles_info_1 = skysports.extract_headlines(check_1, modifier, logger)\n",
    "            articles_info_2 = skysports.extract_headlines(check_2, modifier, logger)\n",
    "\n",
    "        elif domain == 'telegraph':\n",
    "            articles_info_1 = telegraph.extract_headlines(check_1, logger)\n",
    "            articles_info_2 = telegraph.extract_headlines(check_2, logger)\n",
    "        \n",
    "        # Compare on the title and link of the article\n",
    "        return articles_info_1 == articles_info_2\n",
    "        #return articles_info_1['article_title'] == articles_info_2['article_title'] and articles_info_1['article_link'] == articles_info_2['article_link']\n",
    "    \n",
    "    else:\n",
    "        # Else we are looking at stories - will just get the actual text\n",
    "        if domain == 'bbc':\n",
    "            story_details_1 = bbc.get_text(check_1, logger)[0]\n",
    "            story_details_2 = bbc.get_text(check_2, logger)[0]\n",
    "\n",
    "        elif domain == 'dailymail':\n",
    "            story_details_1 = dailymail.get_text(check_1, logger)[0]\n",
    "            story_details_2 = dailymail.get_text(check_2, logger)[0]\n",
    "\n",
    "        elif domain == 'theguardian':\n",
    "            story_details_1 = guardian.get_text(check_1, logger)[0]\n",
    "            story_details_2 = guardian.get_text(check_2, logger)[0]\n",
    "\n",
    "        elif domain == 'mirror':\n",
    "            story_details_1 = mirror.get_text(check_1, logger)[0]\n",
    "            story_details_2 = mirror.get_text(check_2, logger)[0]\n",
    "\n",
    "        elif domain == 'skysports':\n",
    "            story_details_1 = skysports.get_text(check_1, logger)[0]\n",
    "            story_details_2 = skysports.get_text(check_2, logger)[0]\n",
    "\n",
    "        elif domain == 'telegraph':\n",
    "            story_details_1 = telegraph.get_text(check_1, logger)[0]\n",
    "            story_details_2 = telegraph.get_text(check_2, logger)[0]\n",
    "        \n",
    "        return story_details_1 == story_details_2\n",
    "        \n",
    "\n",
    "def check_same(f_name, path_1, path_2, is_pickle, domain, logger):\n",
    "    '''\n",
    "    A function to check if the content found at path_1 and path_2 is the same - note that we \n",
    "    already assume the file names are the same, because only call them in those circumstances\n",
    "    If we are looking at pickles, need to set is_pickle to True and do a pickle load\n",
    "    '''\n",
    "    check_1 = os.path.join(path_1, f_name)\n",
    "    check_2 = os.path.join(path_2, f_name)\n",
    "    \n",
    "    if is_pickle:\n",
    "        with open(check_1, 'rb') as content_1:\n",
    "            with open(check_2,'rb') as content_2:\n",
    "                return pickle.load(content_1) == pickle.load(content_2)\n",
    "    else:\n",
    "        is_sublinks = 'sublinks' in check_1\n",
    "        return check_html(check_1, check_2, domain, is_sublinks, logger)\n",
    "\n",
    "def try_candidates(deletion_candidates, is_pickle, logger, domain, deletion_log = None):\n",
    "    '''\n",
    "    A function to try the candidates to see if they have the same content - and then delete or not accordingly\n",
    "    Will return a frame with the deleted files (to remove from our potential files) and the log, which has been updated\n",
    "    Note that we feed in the deletion log to update it - but if we feed \"None\", it does not get updated\n",
    "    '''\n",
    "    # Can't find a way to do this by the column names - so will get indices\n",
    "    pf_index = deletion_candidates.columns.get_loc('Potential_file')\n",
    "    pp_index = deletion_candidates.columns.get_loc('Potential_path')\n",
    "    fp_index = deletion_candidates.columns.get_loc('File_path')\n",
    "    \n",
    "    # Then get whether or not we should delete them\n",
    "    logger.info('Will check {} files to see if they are the same'.format(deletion_candidates.shape[0]))\n",
    "    deletion_candidates['Delete'] = deletion_candidates.apply(lambda x: check_same(x[pf_index], x[pp_index], x[fp_index], is_pickle, domain, logger), axis = 1)\n",
    "    \n",
    "    # Delete those that we should\n",
    "    logger.info('Have found {} files that will be deleted'.format(deletion_candidates['Delete'].sum()))\n",
    "    deletion_candidates[deletion_candidates['Delete']].apply(lambda x: os.remove(x[pp_index] + x[pf_index]), axis = 1)\n",
    "    \n",
    "    # Then we need to update the deletion log\n",
    "    if deletion_log is not None:\n",
    "        # Pull the name and path, and add whether or not we are dealing with pickles\n",
    "        new_entries = deletion_candidates.loc[deletion_candidates['Delete'], ['File_name', 'File_path']]\n",
    "        new_entries['Is_pickle'] = is_pickle\n",
    "        \n",
    "        # Then append to end\n",
    "        deletion_log = deletion_log.append(new_entries)\n",
    "    \n",
    "    # Finally return the ones we deleted and the log that has been updated\n",
    "    return deletion_candidates[deletion_candidates['Delete']], deletion_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 3: Process\n",
    "\n",
    "The process to follow will be:\n",
    "\n",
    "1. Load file names for a given date (TODAY's date)\n",
    "\n",
    "2. Compare these file names to our \"log\" files\n",
    "\n",
    "3. If not in the \"log\" files, then look for the file in each one of the previous dates, starting from the earliest, until we find a match\n",
    "\n",
    "4. For every match found, check that the HTML content is also equal (note that the same function does these both)\n",
    "\n",
    "5. If a match is found, immediately stop and delete the current file, adding the original file to our log, if not already there\n",
    "\n",
    "Note that we ONLY compare domain to domain - NO cross domain stuff\n",
    "\n",
    "Note that we will initially read in the log files, add to them as we go, and then save at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start off by loading in the deletion logs\n",
    "deletion_log = pd.read_csv(log_file)\n",
    "\n",
    "# Date for testing\n",
    "date_today = '2018_02_02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_name</th>\n",
       "      <th>File_path</th>\n",
       "      <th>Is_pickle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [File_name, File_path, Is_pickle]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deletion_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think maybe could be running into an issue where, even though HTML is the \"same\", pulling it on different dates will give different metadata / adverts etc.\n",
    "\n",
    "I think the pickle part is going OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd\n",
    "\n",
    "def delete_duplicates(search_loc, is_pickle, date_today, past_deletons, logger):\n",
    "    '''\n",
    "    Function for looking at currently saved files and delete them if they are duplicates of something already seen\n",
    "    This will be carried out after saving HTML and getting headlines - focusing on not having to pull more HTML than necessary\n",
    "    Search_loc is the key argument and should point directly to suburls, story_links, or pickle stories\n",
    "    Past deletions should be a directory to where the deletions are located\n",
    "    '''\n",
    "    logger.info('Looking for files in:\\n{}'.format(search_loc))\n",
    "    \n",
    "    # Search ever domain in the directory given to search through\n",
    "    for domain in os.listdir(search_loc):\n",
    "        logger.info('Looking at {}'.format(domain))\n",
    "        \n",
    "        date_loc = os.path.join(search_loc, domain, date_today)\n",
    "            \n",
    "        # The first part of the process is to grab all of the potential files up for deletion\n",
    "        file_frame = pd.DataFrame({'Potential_file' : os.listdir(date_loc), 'Potential_path' : date_loc})\n",
    "        \n",
    "        logger.info('Will be looking at {} files'.format(file_frame.shape[0]))\n",
    "        \n",
    "        # Now the process of deletion is to look at the past deletions (if any) and then all dates that came before\n",
    "        deletion_log = pd.read_csv(past_deletions)\n",
    "        \n",
    "        if deletion_log.shape[0] != 0:\n",
    "            # Look at past deletions to do a quick compare and delete - do this by merging on file names\n",
    "            deletion_candidates = file_frame.merge(deletion_log[deletion_log['Is_pickle'] == is_pickle], left_on = 'Potential_file', right_on = 'File_name')\n",
    "            num_log_delete = deletion_candidates.shape[0]\n",
    "            \n",
    "            logger.info('Have found {} candidates to delete from our log'.format(num_log_delete))\n",
    "            \n",
    "            # So if we have something here, will delete, but won't update our log\n",
    "            if num_log_delete != 0:\n",
    "                deleted_candidates, _ = try_candidates(deletion_candidates, is_pickle, logger, domain)\n",
    "                \n",
    "                # Once we have deleted, will remove those files from our original list\n",
    "                file_frame = file_frame[~ file_frame['Potential_file'].isin(deleted_candidates['Potential_file'])]\n",
    "                \n",
    "                logger.info('After deletion now have {} files left'.format(file_frame.shape[0]))\n",
    "        \n",
    "        # Then we will search through previous dates in that domain\n",
    "        domain_loc = os.path.join(search_loc, domain)\n",
    "        possbile_dates = os.listdir(domain_loc)\n",
    "        valid_dates = sorted([possible_date for possible_date in possbile_dates if datetime.datetime.strptime(possible_date, '%Y_%m_%d') < datetime.datetime.strptime(date_today, '%Y_%m_%d')])\n",
    "        \n",
    "        logger.info('Have found {} dates to search through'.format(len(search_dates)))\n",
    "        \n",
    "        # Now we will loop through the dates and do a similar thing to the deletion log\n",
    "        for deletion_date in possible_dates:\n",
    "            logger.info('Now searching date {}'.format(search_date))\n",
    "            delete_path = os.path.join(search_loc, domain, deletion_date)\n",
    "            \n",
    "            # Since we don't have a deletion log, make a \"check\" frame\n",
    "            check_frame = pd.DataFrame({'File_name' : os.listdir(delete_path), 'File_path' : delete_path, 'Is_pickle' : is_pickle})\n",
    "            \n",
    "            logger.info('Comparing against {} files'.format(check_frame.shape[0]))\n",
    "            \n",
    "            # Then follow the same process and get the candidates\n",
    "            deletion_candidates = file_frame.merge(check_frame, left_on = 'Potential_file', right_on = 'File_name')\n",
    "            num_date_delete = deletion_candidates.shape[0]\n",
    "\n",
    "            # Then delete\n",
    "            logger.info('Have found {} candidates to delete from {}'.format(num_date_delete, deletion_date))\n",
    "\n",
    "            # Again we dive in if we have found some candidates, but this time updating the log\n",
    "            if num_date_delete != 0:\n",
    "                deleted_candidates, deletion_log = try_candidates(deletion_candidates, is_pickle, logger, domain, deletion_log)\n",
    "\n",
    "                # And finally remove like before\n",
    "                file_frame = file_frame[~ file_frame['Potential_file'].isin(deleted_candidates['Potential_file'])]\n",
    "                logger.info('After deletion now have {} files left'.format(file_frame.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] El sistema no puede encontrar la ruta especificada: 'c:/Users/amathewl/Desktop/3_Personal_projects/football/Data_work/Data/01_HTML/bbc\\\\2018_05_29'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ea5b2b02cc5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mfile_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mfile_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfile_type\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate_loc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mfile_type\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'base_urls'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Have added the slash here to avoid a double slash later\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfile_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] El sistema no puede encontrar la ruta especificada: 'c:/Users/amathewl/Desktop/3_Personal_projects/football/Data_work/Data/01_HTML/bbc\\\\2018_05_29'"
     ]
    }
   ],
   "source": [
    "for search_loc in [html_loc, story_loc]:\n",
    "    # So we start off looking in these two locations and will branch slightly depending\n",
    "    is_pickle = search_loc == story_loc\n",
    "    \n",
    "    dev_logger.info('Looking for files in:\\n{}'.format(search_loc))\n",
    "    \n",
    "    for domain in os.listdir(search_loc):\n",
    "        dev_logger.info('Looking at {}'.format(domain))\n",
    "        \n",
    "        # Declare dateloc first\n",
    "        date_loc = os.path.join(search_loc, domain, date_today)\n",
    "        \n",
    "        # If we are not looking at pickles - we have several places to look (but never look at base urls)\n",
    "        if is_pickle:\n",
    "            file_types = ['']\n",
    "        else:\n",
    "            file_types = [file_type + '/' for file_type in os.listdir(date_loc) if file_type != 'base_urls'] # Have added the slash here to avoid a double slash later\n",
    "        \n",
    "        for file_type in file_types:\n",
    "            dev_logger.info('Looking at file type: {}'.format(file_type))\n",
    "            \n",
    "            # We will only look in our \"date today\" for our potential files - will use this as a fixed path for deletion\n",
    "            type_loc = os.path.join(date_loc, file_type)\n",
    "\n",
    "            # Grab the file names and put into a pandas frame so that we can keep track of it\n",
    "            file_frame = pd.DataFrame({'Potential_file' : os.listdir(type_loc), 'Potential_path' : type_loc})\n",
    "\n",
    "            dev_logger.info('Will look at {} files'.format(file_frame.shape[0]))\n",
    "            ## NOW START PROCESS OF DELETION ##\n",
    "\n",
    "            # Grab the corresponding entries from our log - note that we should never be comparing data from the same dates here as log should be updated continuously\n",
    "            deletion_candidates = file_frame.merge(deletion_log[deletion_log['Is_pickle'] == is_pickle], left_on = 'Potential_file', right_on = 'File_name')\n",
    "            num_log_delete = deletion_candidates.shape[0]\n",
    "\n",
    "            # Then process these candidates and remove from our original set of files the ones that get deleted\n",
    "            dev_logger.info('Have found {} candidates to delete from our log'.format(num_log_delete))\n",
    "            \n",
    "            # If we have candidates, then try the content but DON'T update the log\n",
    "            if num_log_delete != 0:\n",
    "                deleted_candidates, _ = try_candidates(deletion_candidates, is_pickle, dev_logger, domain)\n",
    "\n",
    "                # And then remove from our file_frame\n",
    "                file_frame = file_frame[~ file_frame['Potential_file'].isin(deleted_candidates['Potential_file'])]\n",
    "\n",
    "                dev_logger.info('After deletion now have {} files left'.format(file_frame.shape[0]))\n",
    "\n",
    "            # Now we have to look at previous dates so we search through the directories which are named after the dates\n",
    "            domain_loc = os.path.join(search_loc, domain)\n",
    "            directory_dates = os.listdir(domain_loc)\n",
    "            search_dates = sorted([directory_date for directory_date in directory_dates if datetime.datetime.strptime(directory_date, '%Y_%m_%d') < datetime.datetime.strptime(date_today, '%Y_%m_%d')])\n",
    "\n",
    "            dev_logger.info('Have found {} dates to search through'.format(len(search_dates)))\n",
    "\n",
    "            # Then we will loop through them to find potential candidates\n",
    "            for search_date in search_dates:\n",
    "                dev_logger.info('Now searching date {}'.format(search_date))\n",
    "                search_date_loc = os.path.join(domain_loc, search_date, file_type)\n",
    "\n",
    "                # Since we don't have the log frame - make a check frame\n",
    "                check_frame = pd.DataFrame({'File_name' : os.listdir(search_date_loc), 'File_path' : search_date_loc, 'Is_pickle' : is_pickle})\n",
    "\n",
    "                dev_logger.info('Comparing against {} files'.format(check_frame.shape[0]))\n",
    "\n",
    "                # Then get our candidates\n",
    "                deletion_candidates = file_frame.merge(check_frame, left_on = 'Potential_file', right_on = 'File_name')\n",
    "                num_date_delete = deletion_candidates.shape[0]\n",
    "\n",
    "                # Then delete\n",
    "                dev_logger.info('Have found {} candidates to delete from {}'.format(num_date_delete, search_date))\n",
    "                \n",
    "                # Again we dive in if we have found some candidates, but this time updating the log\n",
    "                if num_date_delete != 0:\n",
    "                    deleted_candidates, deletion_log = try_candidates(deletion_candidates, is_pickle, dev_logger, domain, deletion_log)\n",
    "\n",
    "                    # And finally remove\n",
    "                    file_frame = file_frame[~ file_frame['Potential_file'].isin(deleted_candidates['Potential_file'])]\n",
    "                    dev_logger.info('After deletion now have {} files left'.format(file_frame.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally save the deletion log\n",
    "dev_logger.info('Saving deletion log of length {}'.format(deletion_log.shape[0]))\n",
    "deletion_log.to_csv(log_file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_name</th>\n",
       "      <th>File_path</th>\n",
       "      <th>Is_pickle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [File_name, File_path, Is_pickle]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deletion_log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
