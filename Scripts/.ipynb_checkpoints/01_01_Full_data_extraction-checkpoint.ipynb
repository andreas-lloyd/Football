{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full data extraction\n",
    "\n",
    "This is the script where the full data extraction should be done without the need to run various scripts. All the functions will be taken from the *football_functions* package and nothing else should need to be imported.\n",
    "\n",
    "There are several key features to these functions in the way that they save and pull HTML - almost all of them need the current date and proxy to be provided. This is done so that a proxy can easily be dealt with and any date of choice can be used (to go back and fix old data).\n",
    "\n",
    "Note that they will all also ask for paths - this is done so that switching between OS should not be difficult. To that end the system of *HOME_PATH* is also used, such that all paths are dependent on the *HOME_PATH* and only it should need to be modified, as long as the file structure is followed.\n",
    "\n",
    "The important structure is as follows\n",
    "\n",
    "/Data/\n",
    "\n",
    "    00_Organisation/\n",
    "    01_HTML/\n",
    "    02_Stories/\n",
    "    \n",
    "Within each one you will find the domain name as well as the date it was extracted on - and subfolders indicating different processes (such as *base link* or *sublink*).\n",
    "\n",
    "The process is as follows:\n",
    "\n",
    "    1) Pull the HTML from the baselinks (depends on generic.pull_html)\n",
    "    2) Find the sublinks within certain pages of the baselinks, as some are, for example, team pages (depends on source_specific)\n",
    "    3) Scrape all the sublinks found to get our \"main pages\" for the scraping of headlines (depends on generic.pull_html)\n",
    "    4) Find the headlines within the scraped sublinks, saving information about the article and any easy extras (like image titles or summaries) in order to better tag later (depends on source_specific)\n",
    "    5) Scrape the headline URL found in the previous step in order to get the story's HTML for story extraction (depends on generic.pull_html)\n",
    "    6) Get the actual story from the HTML and save it along with the other information *NOT COMPLETE*\n",
    "    \n",
    "Steps depending on *generic.pull_html* will save .HTML files in *Data/01_HTML*. Step 2 saves links in a list under *Data/00_Organisation*, and step 4 saves .pickle files with dictionaries for each story under */Data/02_Stories*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial constants\n",
    "\n",
    "Some initial constants just for the initial overhead, only this should need to be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base packages for running the script\n",
    "import sys, datetime, pickle\n",
    "\n",
    "# Set the path and proxy in accordance with our OS\n",
    "if sys.platform == 'linux':\n",
    "    HOME_PATH = '/home/andreas/Desktop/Projects/Football/'\n",
    "    proxy_settings = None\n",
    "else:\n",
    "    HOME_PATH = 'c:/Users/amathewl/Desktop/3_Personal_projects/football/'\n",
    "    proxy_path = HOME_PATH + 'Data/00_Organisation/' + 'proxy.pickle'\n",
    "    \n",
    "    with open(proxy_path, 'rb') as proxy_file:\n",
    "        proxy_settings = pickle.load(proxy_file)\n",
    "    \n",
    "# Relative paths\n",
    "data_loc = HOME_PATH + 'Data/'\n",
    "html_loc = data_loc + '01_HTML/'\n",
    "organ_loc = data_loc + '00_Organisation/'\n",
    "story_loc = data_loc + '02_Stories/'\n",
    "\n",
    "# Files to find\n",
    "baseurl_loc = organ_loc + 'news_sources.txt'\n",
    "\n",
    "# Get today's date for various functions\n",
    "date_today = datetime.datetime.today().strftime('%Y_%m_%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing from football_functions\n",
    "\n",
    "The personal functions we will use from football_functions for the initial processes. Have separated this because could cause some issues when changes are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logger\n",
    "from football_functions.generic import default_logger\n",
    "\n",
    "# Base URLs\n",
    "from football_functions.processes.html_extraction import baseurls\n",
    "\n",
    "# Suburls\n",
    "from football_functions.processes.html_extraction import suburls\n",
    "\n",
    "# Headlines\n",
    "from football_functions.processes.information_extraction import headlines\n",
    "\n",
    "# Stories\n",
    "from football_functions.processes.information_extraction import stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring the logger\n",
    "\n",
    "Declare the logger we will be using throughout the process to save logs of our stuff instead of printing.\n",
    "Will get a logger for each process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_logger = default_logger.get_logger(data_loc, date_today, 'baseline')\n",
    "suburl_logger = default_logger.get_logger(data_loc, date_today, 'suburl')\n",
    "headline_logger = default_logger.get_logger(data_loc, date_today, 'headline')\n",
    "story_logger = default_logger.get_logger(data_loc, date_today, 'story')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline URL extraction\n",
    "\n",
    "Getting the HTML from the baseline URLs and saving to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_errors = baseurls.scrape_urls(baseurl_loc, html_loc, date_today, proxy_settings, baseline_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suburl extraction\n",
    "\n",
    "Getting the suburls from the HTML in block 3 and then scraping them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "suburls.extract_urls(html_loc, organ_loc, date_today, suburl_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "suburl_errors = suburls.scrape_urls(organ_loc, html_loc, date_today, proxy_settings, suburl_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headline extraction\n",
    "\n",
    "Getting the headlines from all the HTML pages that we have looked at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines.process_html(html_loc, story_loc, date_today, headline_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Story extraction\n",
    "\n",
    "Getting the HTML from the headline links that we found in block 5 - then getting the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_errors = stories.process_articles(story_loc, html_loc, date_today, proxy_settings, story_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_errors = stories.get_articles(story_loc, html_loc, date_today, story_logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
