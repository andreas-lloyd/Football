{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lookback to old data and fix up\n",
    "\n",
    "In this script will cycle through all dates found and then check if we have processed them in a certain way. If not - then will go forward and process them.\n",
    "\n",
    "The parts of the process are:\n",
    "\n",
    "    1) Extract sublinks\n",
    "\n",
    "    2) Pull their HTML\n",
    "\n",
    "    3) Get the headlines\n",
    "\n",
    "    4) Get the stories\n",
    "\n",
    "The best way to do this is to:\n",
    "\n",
    "    1) Go into the HTML folder\n",
    "\n",
    "    2) Look at a given source\n",
    "\n",
    "    3) Look at a given date \n",
    "\n",
    "    4) Look for base_urls -> convert all to .html (name change)\n",
    "    \n",
    "        i) If we don't find sublinks - then spool through HTML to get sublinks\n",
    "\n",
    "    5) Look for suburls -> convert all to .html\n",
    "\n",
    "        i) If we don't find the corresponding /Stories/ part, then need to spool through for stories\n",
    "        ii) If there is no stories link part, then pull the HTML from those story links\n",
    "        iii) If we do not find the story text + other parts generated by the story extraction, run that too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 1: Initial constants\n",
    "\n",
    "Want to give some of the basic paths to let us check through the directories. Dates and such should be automatically looped through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base packages for running the script\n",
    "import sys, os, re, pickle, datetime\n",
    "\n",
    "# Set the path and proxy in accordance with our OS\n",
    "if sys.platform == 'linux':\n",
    "    HOME_PATH = '/home/andreas/Desktop/Projects/Football/'\n",
    "    proxy_settings = None\n",
    "else:\n",
    "    HOME_PATH = 'c:/Users/amathewl/Desktop/3_Personal_projects/football_old/football/'\n",
    "    proxy_path = HOME_PATH + 'Data/00_Organisation/' + 'proxy.pickle'\n",
    "    \n",
    "    with open(proxy_path, 'rb') as proxy_file:\n",
    "        proxy_settings = pickle.load(proxy_file)\n",
    "    \n",
    "# Relative paths\n",
    "data_loc = HOME_PATH + 'Data/'\n",
    "html_loc = data_loc + '01_HTML/'\n",
    "organ_loc = data_loc + '00_Organisation/'\n",
    "story_loc = data_loc + '02_Stories/'\n",
    "\n",
    "# End names of directories that we will be checking for\n",
    "base_urls = 'base_urls'\n",
    "sub_links = 'sublinks'\n",
    "story_links = 'story_link'\n",
    "\n",
    "# Get today's date for our logs\n",
    "date_today = datetime.datetime.today().strftime('%Y_%m_%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger\n",
    "from football_functions.generic import default_logger\n",
    "\n",
    "# Base URLs\n",
    "from football_functions.processes.html_extraction import baseurls\n",
    "\n",
    "# Suburls\n",
    "from football_functions.processes.html_extraction import suburls\n",
    "\n",
    "# Headlines\n",
    "from football_functions.processes.information_extraction import headlines\n",
    "\n",
    "# Stories\n",
    "from football_functions.processes.information_extraction import stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will use this as a master logger for everything instead of separating out\n",
    "lookback_logger = default_logger.get_logger(data_loc, date_today, 'lookback')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 2: Function definitions\n",
    "\n",
    "Some basic functions that aren't really worth putting in the package as I will only be using them here. Will do stuff like checking if stuff exists to tell us if we should call some part of the full process.\n",
    "\n",
    "Function that goes into a directory and converts some ending into another (.txt to .html)\n",
    "\n",
    "Would be good to have a function that calls whole \"ends\" of the full process given some numeric input, e.g. f(1) would call the whole thing minus the first step, f(2) would do everything after the second step etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_html(directory_loc, logger):\n",
    "    '''\n",
    "    Function to convert all files in a directory to .html (just changing the name)\n",
    "    '''\n",
    "    all_files = os.listdir(directory_loc)\n",
    "    \n",
    "    files_to_convert = [txt_file for txt_file in all_files if '.txt' in txt_file]\n",
    "    \n",
    "    logger.info('Renaming {} files out of {} found in \\n{}'.format(len(files_to_convert), len(all_files), directory_loc))\n",
    "    \n",
    "    for file_name in files_to_convert:\n",
    "        old_path = directory_loc + '/' + file_name\n",
    "        new_path = re.sub(r'\\.txt$', '.html', old_path)\n",
    "        \n",
    "        os.rename(old_path, new_path)\n",
    "    \n",
    "    logger.info('Finished renaming files')\n",
    "\n",
    "def full_process(step, date_today, domain, logger):\n",
    "    '''\n",
    "    Function that will do the whole process for us, depending on what step we say\n",
    "    '''\n",
    "    if step <= 1:\n",
    "        # Find and scrape subrls (note that we always did this together)\n",
    "        logger.info('Entering step 1')\n",
    "        suburls.extract_urls(html_loc, organ_loc, date_today, logger, domain)\n",
    "        suburl_errors = suburls.scrape_urls(organ_loc, html_loc, date_today, proxy_settings, logger)\n",
    "        \n",
    "    if step <= 2:\n",
    "        # Get headlines from within suburls\n",
    "        logger.info('Entering step 2')\n",
    "        headlines.process_html(html_loc, story_loc, date_today, logger, domain)\n",
    "        \n",
    "    if step <= 3:\n",
    "        # Get the html from story links\n",
    "        logger.info('Entering step 3')\n",
    "        story_errors = stories.process_articles(story_loc, html_loc, date_today, proxy_settings, logger, domain)\n",
    "    \n",
    "    if step <= 4:\n",
    "        # Get the story from the html\n",
    "        logger.info('Entering step 4')\n",
    "        text_errors = stories.get_articles(story_loc, html_loc, date_today, logger, domain)\n",
    "    \n",
    "    # In case we never entered step 3\n",
    "    if step == 4:\n",
    "        story_errors = None\n",
    "    \n",
    "    return story_errors, text_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 3: Moving through sources and dates\n",
    "The basic loop through of sources and dates, looking for certain things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that some do not have suburls - so will exclude them, to avoid repeating something we don't want to do\n",
    "no_suburls = ['skysports', 'telegraph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbc\n",
      "2018_01_04\n",
      "bbc\n",
      "2018_01_08\n",
      "bbc\n",
      "2018_01_09\n",
      "bbc\n",
      "2018_01_10\n",
      "3\n",
      "bbc\n",
      "2018_01_11\n",
      "3\n",
      "bbc\n",
      "2018_01_12\n",
      "4\n",
      "bbc\n",
      "2018_01_14\n",
      "4\n",
      "bbc\n",
      "2018_01_16\n",
      "4\n",
      "bbc\n",
      "2018_01_17\n",
      "4\n",
      "bbc\n",
      "2018_01_18\n",
      "4\n",
      "bbc\n",
      "2018_01_19\n",
      "4\n",
      "bbc\n",
      "2018_01_22\n",
      "4\n",
      "bbc\n",
      "2018_01_23\n",
      "4\n",
      "dailymail\n",
      "2018_01_04\n",
      "2\n",
      "dailymail\n",
      "2018_01_08\n",
      "2\n",
      "dailymail\n",
      "2018_01_09\n",
      "2\n",
      "dailymail\n",
      "2018_01_10\n",
      "2\n",
      "dailymail\n",
      "2018_01_11\n",
      "3\n",
      "dailymail\n",
      "2018_01_12\n",
      "4\n",
      "dailymail\n",
      "2018_01_14\n",
      "3\n",
      "dailymail\n",
      "2018_01_16\n",
      "4\n",
      "dailymail\n",
      "2018_01_17\n",
      "4\n",
      "dailymail\n",
      "2018_01_18\n",
      "4\n",
      "dailymail\n",
      "2018_01_19\n",
      "4\n",
      "dailymail\n",
      "2018_01_22\n",
      "4\n",
      "dailymail\n",
      "2018_01_23\n",
      "4\n",
      "mirror\n",
      "2018_01_04\n",
      "2\n",
      "mirror\n",
      "2018_01_08\n",
      "2\n",
      "mirror\n",
      "2018_01_09\n",
      "2\n",
      "mirror\n",
      "2018_01_10\n",
      "2\n",
      "mirror\n",
      "2018_01_11\n",
      "3\n",
      "mirror\n",
      "2018_01_12\n",
      "4\n",
      "mirror\n",
      "2018_01_14\n",
      "3\n",
      "mirror\n",
      "2018_01_16\n",
      "4\n",
      "mirror\n",
      "2018_01_17\n",
      "4\n",
      "mirror\n",
      "2018_01_18\n",
      "4\n",
      "mirror\n",
      "2018_01_19\n",
      "4\n",
      "mirror\n",
      "2018_01_22\n",
      "4\n",
      "mirror\n",
      "2018_01_23\n",
      "4\n",
      "skysports\n",
      "2018_01_04\n",
      "2\n",
      "skysports\n",
      "2018_01_08\n",
      "2\n",
      "skysports\n",
      "2018_01_09\n",
      "2\n",
      "skysports\n",
      "2018_01_10\n",
      "2\n",
      "skysports\n",
      "2018_01_11\n",
      "3\n",
      "skysports\n",
      "2018_01_12\n",
      "4\n",
      "skysports\n",
      "2018_01_14\n",
      "3\n",
      "skysports\n",
      "2018_01_16\n",
      "4\n",
      "skysports\n",
      "2018_01_17\n",
      "4\n",
      "skysports\n",
      "2018_01_18\n",
      "4\n",
      "skysports\n",
      "2018_01_19\n",
      "4\n",
      "skysports\n",
      "2018_01_22\n",
      "4\n",
      "skysports\n",
      "2018_01_23\n",
      "4\n",
      "telegraph\n",
      "2018_01_04\n",
      "2\n",
      "telegraph\n",
      "2018_01_08\n",
      "2\n",
      "telegraph\n",
      "2018_01_09\n",
      "2\n",
      "telegraph\n",
      "2018_01_10\n",
      "2\n",
      "telegraph\n",
      "2018_01_11\n",
      "3\n",
      "telegraph\n",
      "2018_01_12\n",
      "4\n",
      "telegraph\n",
      "2018_01_14\n",
      "3\n",
      "telegraph\n",
      "2018_01_16\n",
      "4\n",
      "telegraph\n",
      "2018_01_17\n",
      "4\n",
      "telegraph\n",
      "2018_01_18\n",
      "4\n",
      "telegraph\n",
      "2018_01_19\n",
      "4\n",
      "telegraph\n",
      "2018_01_22\n",
      "4\n",
      "telegraph\n",
      "2018_01_23\n",
      "4\n",
      "theguardian\n",
      "2018_01_04\n",
      "2\n",
      "theguardian\n",
      "2018_01_08\n",
      "2\n",
      "theguardian\n",
      "2018_01_09\n",
      "2\n",
      "theguardian\n",
      "2018_01_10\n",
      "2\n",
      "theguardian\n",
      "2018_01_11\n",
      "3\n",
      "theguardian\n",
      "2018_01_12\n",
      "4\n",
      "theguardian\n",
      "2018_01_14\n",
      "3\n",
      "theguardian\n",
      "2018_01_16\n",
      "4\n",
      "theguardian\n",
      "2018_01_17\n",
      "4\n",
      "theguardian\n",
      "2018_01_18\n",
      "4\n",
      "theguardian\n",
      "2018_01_19\n",
      "4\n",
      "theguardian\n",
      "2018_01_22\n",
      "4\n",
      "theguardian\n",
      "2018_01_23\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Note that we principally loop the HTML - we will only enter others for checking\n",
    "for source in os.listdir(html_loc):\n",
    "    html_source = html_loc + source\n",
    "    for data_date in os.listdir(html_source):\n",
    "        print(source)\n",
    "        print(data_date)\n",
    "        \n",
    "        html_date = html_source + '/' + data_date\n",
    "        \n",
    "        lookback_logger.info('Working with data found in:\\n{}'.format(html_date))\n",
    "        \n",
    "        # We won't actually repull the baseurls because cannot be done retroactively - only rename .txt to .html\n",
    "        lookback_logger.info('Converting files...')\n",
    "        convert_html(html_date + '/' + base_urls, lookback_logger)\n",
    "        \n",
    "        if os.path.exists(html_date + '/' + sub_links) or source in no_suburls:\n",
    "            \n",
    "            if os.path.exists(html_date + '/' + sub_links):\n",
    "                lookback_logger.info('Have found sublinks - converting files...')\n",
    "\n",
    "                # We have already pulled sublinks and have the HTML - so will convert extensions\n",
    "                convert_html(html_date + '/' + sub_links, lookback_logger)\n",
    "            \n",
    "            # Next step is to check if we have the story in the stories part\n",
    "            if os.path.exists(story_loc + '/' + source + '/' + data_date):\n",
    "                lookback_logger.info('Have found stories')\n",
    "                \n",
    "                # Then we have already pulled the stories from the HTML found in the sublink\n",
    "                \n",
    "                # Next step is to check if we find the stories_link for the HTML\n",
    "                if os.path.exists(html_date + '/' + story_links):\n",
    "                    lookback_logger.info('Have found story HTML')\n",
    "                    \n",
    "                    # Now finally need to check if we ever actually pulled the story\n",
    "                    pickle_loc = story_loc + source + '/' + data_date + '/'\n",
    "                    \n",
    "                    # Get an example story and check that we have story text\n",
    "                    possible_pickles = [possible_pickle for possible_pickle in os.listdir(pickle_loc) if 'fake_link' not in possible_pickle]\n",
    "                    example_loc = pickle_loc + possible_pickles[0]\n",
    "                    with open(example_loc, 'rb') as example_file:\n",
    "                        example_story = pickle.load(example_file)\n",
    "                        \n",
    "                    if 'story_text' in example_story:\n",
    "                        lookback_logger.info('Found story text - everything has already been done!')\n",
    "                    else:\n",
    "                        print('4')\n",
    "                        errors = full_process(4, data_date, [source], lookback_logger)\n",
    "                    \n",
    "                else:\n",
    "                    # Need to pull the HTML from the story link and get the story\n",
    "                    print('3')\n",
    "                    errors = full_process(3, data_date, [source], lookback_logger)\n",
    "                    \n",
    "            else:\n",
    "                # Need to pull the story headlines from the sublink url\n",
    "                print('2')\n",
    "                errors = full_process(2, data_date, [source], lookback_logger)\n",
    "                \n",
    "        else:\n",
    "            # So we need to pull the sublinks from the base_urls html and them pull the HTML - i.e. whole process from there\n",
    "            print('1')\n",
    "            errors = full_process(1, data_date, [source], lookback_logger)\n",
    "        \n",
    "        lookback_logger.info('Have finished with the current date\\n\\n\\n')\n",
    "        \n",
    "    # Print some whitespace just to be clear where we have ended\n",
    "    lookback_logger.info('Have finished with the current source\\n\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
