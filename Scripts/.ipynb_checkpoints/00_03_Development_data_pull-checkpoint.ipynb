{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development\n",
    "\n",
    "This script is for development of other functions - just for simplicity of execution etc. Later the code should be moved to *football_functions*, and then deleted from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 0: Initial packages and definitions\n",
    "\n",
    "Just a block to define some stuff that we will probably never be changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base packages for running the script\n",
    "import sys, datetime\n",
    "\n",
    "# Set the path and proxy in accordance with our OS\n",
    "if sys.platform == 'linux':\n",
    "    HOME_PATH = '/home/andreas/Desktop/Projects/Football/'\n",
    "    proxy_settings = None\n",
    "else:\n",
    "    HOME_PATH = 'c:/Users/amathewl/Desktop/3_Personal_projects/football/'\n",
    "    proxy_settings = None\n",
    "    \n",
    "# Relative paths\n",
    "data_loc = HOME_PATH + 'Data_work/Data/'\n",
    "html_loc = data_loc + '01_HTML/'\n",
    "organ_loc = data_loc + '00_Organisation/'\n",
    "story_loc = data_loc + '02_Stories/'\n",
    "\n",
    "# Get today's date for various functions\n",
    "date_today = datetime.datetime.today().strftime('%Y_%m_%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a logger for development\n",
    "from football_functions.generic import default_logger\n",
    "\n",
    "dev_logger = default_logger.get_logger(data_loc, date_today, 'development')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to pull the data we have saved into a pandas and save to CSV\n",
    "\n",
    "This is the first step in the data analysis phase and just consists of the initial data pull, where we will load all the headlines and/or stories that we have saved on file into a pandas. This probably won't be run too many times, but is good to keep repeatable for when we add stories.\n",
    "\n",
    "Won't do too much initial data processing - but could quite easily add some tags like the source, date pulled, URL of the story etc. Also important to properly convert the encoding and stuff such that we are left with something relatively clean that we don't have to fiddle about with too much.\n",
    "\n",
    "Don't think we will save anything to pickle as it will just eat up too much memory - CSV should be enough.\n",
    "\n",
    "The general process can be quite easily done, as transforming a dictionary into a PD is very easy - just need to select the elements we want, and concat it with another pandas frame that we start with initially. The only slight difference is that we MUST pass an index (story ID) to the PD when we declare it for the concat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "\n",
    "Not sure what functions we will need as it may just be easier as a series of loops and that's it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "The process to follow will be:\n",
    "\n",
    "1. Loop over domains / dates\n",
    "\n",
    "2. Pull the pickles in order\n",
    "\n",
    "3. Declare the dictionary and add any tags we want - including story ID - also decide if want story or not\n",
    "\n",
    "4. Add the dictionary to our data frame\n",
    "\n",
    "5. Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at bbc\n",
      "Looking at dailymail\n",
      "Looking at mirror\n",
      "Looking at skysports\n",
      "Looking at telegraph\n",
      "Looking at theguardian\n"
     ]
    }
   ],
   "source": [
    "all_stories = None\n",
    "story_id = 0\n",
    "\n",
    "for domain in os.listdir(story_loc):\n",
    "    print('Looking at {}'.format(domain))\n",
    "    for date_pulled in os.listdir(os.path.join(story_loc, domain)):\n",
    "        # Where we will be looking for the pickle\n",
    "        pickle_loc = os.path.join(story_loc, domain, date_pulled)\n",
    "        \n",
    "        # Load the pickles in order\n",
    "        for pickle_file in os.listdir(pickle_loc):\n",
    "            with open(os.path.join(pickle_loc, pickle_file), 'rb') as story_file:\n",
    "                story_info = pickle.load(story_file)\n",
    "            \n",
    "            # Have to replace empty with None\n",
    "            for key in story_info:\n",
    "                if story_info[key] == []:\n",
    "                    story_info[key] = None\n",
    "            \n",
    "            # Get the columns for our DF\n",
    "            story_id += 1\n",
    "            to_add = pd.DataFrame({\n",
    "                'Domain' : domain,\n",
    "                'URL' : story_info['article_link'],\n",
    "                'Headline' : story_info['article_title'],\n",
    "                'Summary' : story_info['article_summary'],\n",
    "                'Image' : story_info['article_image'],\n",
    "                'Date' : story_info['article_date'] \n",
    "            }, index = [story_id])\n",
    "            \n",
    "            # And add to our master frame\n",
    "            if all_stories is not None:\n",
    "                all_stories = pd.concat([all_stories, to_add], axis = 0)\n",
    "            else:\n",
    "                all_stories = to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13889, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stories.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stories.to_csv(os.path.join(data_loc, 'all_stories.csv'), index = False, encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
