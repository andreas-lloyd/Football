{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development\n",
    "\n",
    "This script is for development of other functions - just for simplicity of execution etc. Later the code should be moved to *football_functions*, and then deleted from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 0: Initial packages and definitions\n",
    "\n",
    "Just a block to define some stuff that we will probably never be changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base packages for running the script\n",
    "import sys, datetime\n",
    "\n",
    "# Set the path and proxy in accordance with our OS\n",
    "if sys.platform == 'linux':\n",
    "    HOME_PATH = '/home/andreas/Desktop/Projects/Football/'\n",
    "    proxy_settings = None\n",
    "else:\n",
    "    HOME_PATH = 'c:/Users/amathewl/Desktop/03_Personal_projects/football/'\n",
    "    proxy_settings = None\n",
    "    \n",
    "# Relative paths\n",
    "data_log = HOME_PATH + 'Data/'\n",
    "html_loc = data_log + '01_HTML/'\n",
    "organ_loc = data_log + '00_Organisation/'\n",
    "story_loc = data_log + '02_Stories/'\n",
    "\n",
    "# Get today's date for various functions\n",
    "date_today = datetime.datetime.today().strftime('%Y_%m_%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'logger'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e396f81692a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define a logger for development\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfootball_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefault_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdev_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_today\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'development'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Projects/Football/Scripts/football_functions/generic/default_logger.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mJust\u001b[0m \u001b[0ma\u001b[0m \u001b[0mset\u001b[0m \u001b[0mup\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0musing\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgeneral\u001b[0m \u001b[0mthroughout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m '''\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_today\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'logger'"
     ]
    }
   ],
   "source": [
    "# Define a logger for development\n",
    "from football_functions.generic import default_logger\n",
    "\n",
    "dev_logger = default_logger(data_loc, date_today, 'development')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to pull the data we have saved into a pandas and save to CSV\n",
    "\n",
    "This is the first step in the data analysis phase and just consists of the initial data pull, where we will load all the headlines and/or stories that we have saved on file into a pandas. This probably won't be run too many times, but is good to keep repeatable for when we add stories.\n",
    "\n",
    "Won't do too much initial data processing - but could quite easily add some tags like the source, date pulled, URL of the story etc. Also important to properly convert the encoding and stuff such that we are left with something relatively clean that we don't have to fiddle about with too much.\n",
    "\n",
    "Don't think we will save anything to pickle as it will just eat up too much memory - CSV should be enough.\n",
    "\n",
    "The general process can be quite easily done, as transforming a dictionary into a PD is very easy - just need to select the elements we want, and concat it with another pandas frame that we start with initially. The only slight difference is that we MUST pass an index (story ID) to the PD when we declare it for the concat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  4  2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'a' : 4, 'b' : 2}, index = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  4  2\n",
       "0  4  2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pd.DataFrame({'a' : [4], 'b' : [2]}), pd.DataFrame({'a' : [4], 'b' : [2]})], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "\n",
    "Not sure what functions we will need as it may just be easier as a series of loops and that's it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "The process to follow will be:\n",
    "\n",
    "1. Loop over domains / dates\n",
    "\n",
    "2. Pull the pickles in order\n",
    "\n",
    "3. Declare the dictionary and add any tags we want - including story ID - also decide if want story or not\n",
    "\n",
    "4. Add the dictionary to our data frame\n",
    "\n",
    "5. Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain in os.listdir(story_loc):\n",
    "    for date_pulled in os.listdir(os.path.join(story_loc, domain)):\n",
    "        # Where we will be looking for the pickle\n",
    "        pickle_loc = os.path.join(story_loc, domain, date_pulled)\n",
    "        \n",
    "        # Load the pickles in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for files in:\n",
      "/home/andreas/Desktop/Projects/Football/Data/01_HTML/\n",
      "Looking at mirror\n",
      "Looking at file type: sublinks/\n",
      "Will look at 20 files\n",
      "Have found 0 candidates to delete from our log\n",
      "Have found 0 dates to search through\n",
      "Looking at file type: story_link/\n",
      "Will look at 400 files\n",
      "Have found 0 candidates to delete from our log\n",
      "Have found 0 dates to search through\n",
      "Looking at bbc\n",
      "Looking at file type: sublinks/\n",
      "Will look at 158 files\n",
      "Have found 0 candidates to delete from our log\n",
      "Have found 0 dates to search through\n",
      "Looking at file type: story_link/\n",
      "Will look at 1291 files\n",
      "Have found 0 candidates to delete from our log\n",
      "Have found 0 dates to search through\n",
      "Looking at dailymail\n",
      "Looking at file type: sublinks/\n",
      "Will look at 24 files\n",
      "Have found 0 candidates to delete from our log\n",
      "Have found 0 dates to search through\n",
      "Looking at file type: story_link/\n",
      "Will look at 153 files\n",
      "Have found 0 candidates to delete from our log\n",
      "Have found 0 dates to search through\n",
      "Looking at theguardian\n",
      "Looking at file type: sublinks/\n",
      "Will look at 212 files\n",
      "Have found 0 candidates to delete from our log\n",
      "Have found 0 dates to search through\n",
      "Looking at file type: story_link/\n",
      "Will look at 2442 files\n",
      "Have found 0 candidates to delete from our log\n",
      "Have found 0 dates to search through\n",
      "Looking at telegraph\n",
      "Looking at file type: story_link/\n",
      "Will look at 16 files\n",
      "Have found 0 candidates to delete from our log\n",
      "Have found 0 dates to search through\n",
      "Looking at skysports\n",
      "Looking at file type: story_link/\n",
      "Will look at 16 files\n",
      "Have found 0 candidates to delete from our log\n",
      "Have found 0 dates to search through\n",
      "Looking for files in:\n",
      "/home/andreas/Desktop/Projects/Football/Data/02_Stories/\n",
      "Looking at mirror\n",
      "Looking at file type: \n",
      "Will look at 400 files\n",
      "Have found 0 candidates to delete from our log\n",
      "Have found 0 dates to search through\n",
      "Looking at bbc\n",
      "Looking at file type: \n",
      "Will look at 1292 files\n",
      "Have found 0 candidates to delete from our log\n",
      "Have found 0 dates to search through\n",
      "Looking at dailymail\n",
      "Looking at file type: \n",
      "Will look at 153 files\n",
      "Have found 0 candidates to delete from our log\n",
      "Have found 0 dates to search through\n",
      "Looking at theguardian\n",
      "Looking at file type: \n",
      "Will look at 2442 files\n",
      "Have found 0 candidates to delete from our log\n",
      "Have found 0 dates to search through\n",
      "Looking at telegraph\n",
      "Looking at file type: \n",
      "Will look at 16 files\n",
      "Have found 0 candidates to delete from our log\n",
      "Have found 0 dates to search through\n",
      "Looking at skysports\n",
      "Looking at file type: \n",
      "Will look at 60 files\n",
      "Have found 0 candidates to delete from our log\n",
      "Have found 0 dates to search through\n"
     ]
    }
   ],
   "source": [
    "for search_loc in [html_loc, story_loc]:\n",
    "    # So we start off looking in these two locations and will branch slightly depending\n",
    "    is_pickle = search_loc == story_loc\n",
    "    \n",
    "    dev_logger.info('Looking for files in:\\n{}'.format(search_loc))\n",
    "    \n",
    "    for domain in os.listdir(search_loc):\n",
    "        dev_logger.info('Looking at {}'.format(domain))\n",
    "        \n",
    "        # If we are not looking at pickles - we have several places to look (but never look at base urls)\n",
    "        if is_pickle:\n",
    "            file_types = ['']\n",
    "        else:\n",
    "            file_types = [file_type + '/' for file_type in os.listdir(search_loc + domain + '/' + date_today + '/') if file_type != 'base_urls'] # Have added the slash here to avoid a double slash later\n",
    "        \n",
    "        for file_type in file_types:\n",
    "            dev_logger.info('Looking at file type: {}'.format(file_type))\n",
    "            \n",
    "            # We will only look in our \"date today\" for our potential files - will use this as a fixed path for deletion\n",
    "            date_loc = search_loc + domain + '/' + date_today + '/' + file_type\n",
    "\n",
    "            # Grab the file names and put into a pandas frame so that we can keep track of it\n",
    "            file_frame = pd.DataFrame({'Potential_file' : os.listdir(date_loc), 'Potential_path' : date_loc})\n",
    "\n",
    "            dev_logger.info('Will look at {} files'.format(file_frame.shape[0]))\n",
    "            ## NOW START PROCESS OF DELETION ##\n",
    "\n",
    "            # Grab the corresponding entries from our log - note that we should never be comparing data from the same dates here as log should be updated continuously\n",
    "            deletion_candidates = file_frame.merge(deletion_log[deletion_log['Is_pickle'] == is_pickle], left_on = 'Potential_file', right_on = 'File_name')\n",
    "            num_log_delete = deletion_candidates.shape[0]\n",
    "\n",
    "            # Then process these candidates and remove from our original set of files the ones that get deleted\n",
    "            dev_logger.info('Have found {} candidates to delete from our log'.format(num_log_delete))\n",
    "            \n",
    "            # If we have candidates, then try the content but DON'T update the log\n",
    "            if num_log_delete != 0:\n",
    "                deleted_candidates, _ = try_candidates(deletion_candidates, is_pickle)\n",
    "\n",
    "                # And then remove from our file_frame\n",
    "                file_frame = file_frame[~ file_frame['Potential_file'].isin(deleted_candidates['Potential_file'])]\n",
    "\n",
    "                dev_logger.info('After deletion now have {} files left'.format(file_frame.shape[0]))\n",
    "\n",
    "            # Now we have to look at previous dates so we search through the directories which are named after the dates\n",
    "            domain_loc = search_loc + domain + '/'\n",
    "            directory_dates = os.listdir(domain_loc)\n",
    "            search_dates = sorted([directory_date for directory_date in directory_dates if datetime.datetime.strptime(directory_date, '%Y_%m_%d') < datetime.datetime.strptime(date_today, '%Y_%m_%d')])\n",
    "\n",
    "            dev_logger.info('Have found {} dates to search through'.format(len(search_dates)))\n",
    "\n",
    "            # Then we will loop through them to find potential candidates\n",
    "            for search_date in search_dates:\n",
    "                dev_logger.info('Now searching date {}'.format(search_date))\n",
    "                search_date_loc = domain_loc + search_date + '/' + file_type\n",
    "\n",
    "                # Since we don't have the log frame - make a check frame\n",
    "                check_frame = pd.DataFrame({'File_name' : os.listdir(search_date_loc), 'File_path' : search_date_loc, 'Is_pickle' : is_pickle})\n",
    "\n",
    "                dev_logger.info('Comparing against {} files'.format(check_frame.shape[0]))\n",
    "\n",
    "                # Then get our candidates\n",
    "                deletion_candidates = file_frame.merge(check_frame, left_on = 'Potential_file', right_on = 'File_name')\n",
    "                num_log_delete = deletion_candidates.shape[0]\n",
    "\n",
    "                # Then delete\n",
    "                dev_logger.info('Have found {} candidates to delete from our log'.format(num_log_delete))\n",
    "                \n",
    "                # Again we dive in if we have found some candidates, but this time updating the log\n",
    "                if num_log_delete != 0:\n",
    "                    deleted_candidates, deletion_log = try_candidates(deletion_candidates, is_pickle, deletion_log)\n",
    "\n",
    "                    # And finally remove\n",
    "                    file_frame = file_frame[~ file_frame['Potential_file'].isin(deleted_candidates['Potential_file'])]\n",
    "                    dev_logger.info('After deletion now have {} files left'.format(file_frame.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally save the deletion log\n",
    "dev_logger.info('Saving deletion log of length {}'.format(deletion_log.shape[0]))\n",
    "deletion_log.to_csv(log_file, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
