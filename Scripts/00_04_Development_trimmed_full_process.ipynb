{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development\n",
    "\n",
    "This script is for development of other functions - just for simplicity of execution etc. Later the code should be moved to *football_functions*, and then deleted from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 0: Initial packages and definitions\n",
    "\n",
    "Just a block to define some stuff that we will probably never be changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base packages for running the script\n",
    "import sys, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the path and proxy in accordance with our OS\n",
    "if sys.platform == 'linux':\n",
    "    HOME_PATH = Path('/home/andreas/Desktop/Projects/Football/')\n",
    "    proxy_settings = None\n",
    "else:\n",
    "    HOME_PATH = Path('c:/Users/amathewl/Desktop/3_Personal_projects/football/')\n",
    "    proxy_settings = None\n",
    "    \n",
    "# Relative paths\n",
    "data_loc = HOME_PATH / 'Data'\n",
    "html_loc = data_loc / '01_HTML'\n",
    "organ_loc = data_loc / '00_Organisation'\n",
    "story_loc = data_loc / '02_Stories'\n",
    "\n",
    "# Get today's date for various functions\n",
    "date_today = datetime.datetime.today().strftime('%Y_%m_%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a logger for development\n",
    "from football_functions.generic import default_logger\n",
    "\n",
    "dev_logger = default_logger.get_logger(data_loc, date_today, 'development')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to do the full process, but all in one go\n",
    "\n",
    "Goal is to trim down the full process and do everything in \"one go\", only saving the final pickle and no HTML. This would be something we would use on a server online where space is more important. The process would be:\n",
    "\n",
    "* Load in the base URLs\n",
    "\n",
    "* Loop over each URL - starting with pulling the HTML\n",
    "\n",
    "* Then with the HTML in memory, look for sublinks\n",
    "\n",
    "* Then we loop over the sublinks (or the original HTML if it is \"not\" a sublink type URL)\n",
    "\n",
    "* Pull the headlines from the sublinks - loop over these and get the story\n",
    "\n",
    "* Finally save the headline and everything into a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "\n",
    "Need a function to do each step as well as pull them all together.\n",
    "\n",
    "I think want to have the initial file load in our grand function - and then scrape HTML elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time, random, requests\n",
    "\n",
    "def request_html(url, proxy, logger):\n",
    "    \"\"\"\n",
    "    Function that is fed a URL and then tries to pull from it, doing some error checking and handling proxies + sleeping\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pull the HTML data from the URL using requests\n",
    "    try:\n",
    "        response = requests.get(url, proxies = proxy)\n",
    "        \n",
    "        # If we 404 then we have connected well\n",
    "        if response.status_code == 404:\n",
    "            logger.error('404 Error')\n",
    "            return (url, '404 error')\n",
    "        else:\n",
    "            html_text = response.text\n",
    "            logger.debug('Successfully pulled HTML from {}'.format(url))\n",
    "            return (html_text, 'No error')\n",
    "    except requests.exceptions.RequestException as error:\n",
    "        # The link timed out\n",
    "        logger.warning('Have not managed to pull from {} due to a {}'.format(url, error))\n",
    "        logger.debug('Sleeping for 5 seconds to see if works again')\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Try again after sleeping for 5 seconds in case we were rejected\n",
    "        try:\n",
    "            response = requests.get(url, proxies = proxy)\n",
    "            \n",
    "            if response.status_code == 404:\n",
    "                logger.error('404 Error')\n",
    "                return (url, '404 error')\n",
    "            else:\n",
    "                html_text = response.text\n",
    "                logger.debug('Successfully pulled HTML from {}'.format(url))\n",
    "                return (html_text, 'No error')\n",
    "        except requests.exceptions.RequestException as error:\n",
    "            logger.error('Still not working')\n",
    "            return (url, error)\n",
    "\n",
    "def process_url(url, proxy, logger):\n",
    "    \"\"\"\n",
    "    A function that is given a URL and then checks it over to see if worth scraping\n",
    "    If valid, will proceed to scrape - so all scraping goes here\n",
    "    Note that if we produce an error, will return the URL again\n",
    "    \"\"\"\n",
    "    logger.info('Scraping URL {}'.format(url))\n",
    "    \n",
    "    # First check if the think we are scraping is valid and remove any spaces\n",
    "    valid_url = re.match('^http\\S*www\\.\\S*$', url)\n",
    "    \n",
    "    if valid_url and 'fake_link' not in url:\n",
    "        # Always sleep a bit before requesting, just to stop us being rejected so much\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "        \n",
    "        return request_html(valid_url.group(0).rstrip(), proxy, logger)\n",
    "    else:\n",
    "        logger.warning('The link found is not valid\\n {}\\n'.format(url))\n",
    "        return (url, 'Invalid URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import football_functions.reduced.source_specific.bbc.process_html as bbc\n",
    "import football_functions.reduced.source_specific.dailymail.process_html as dailymail\n",
    "import football_functions.reduced.source_specific.mirror.process_html as mirror\n",
    "import football_functions.reduced.source_specific.guardian.process_html as guardian\n",
    "import football_functions.reduced.source_specific.skysports.process_html as skysports\n",
    "import football_functions.reduced.source_specific.telegraph.process_html as telegraph\n",
    "\n",
    "def find_suburls(baseurl_html, base_url, domain, logger):\n",
    "    \"\"\"\n",
    "    Function that will return to us all of the suburls on the page we just downloaded\n",
    "    \"\"\"\n",
    "    links = []\n",
    "\n",
    "    # Will only process certain URLsfor each domain\n",
    "    if domain == 'bbc' and 'teams' in base_url:\n",
    "        links.extend(bbc.get_suburls(baseurl_html, logger))\n",
    "\n",
    "    elif domain == 'dailymail' and 'sport/football' in base_url:\n",
    "        links.extend(dailymail.get_suburls(baseurl_html, logger))\n",
    "\n",
    "    elif domain == 'mirror'  and 'sport/football' in base_url:\n",
    "        links.extend(mirror.get_suburls(baseurl_html, logger))\n",
    "\n",
    "    elif domain == 'theguardian'  and 'teams' in base_url:\n",
    "        links.extend(guardian.get_suburls(baseurl_html, logger))\n",
    "    else:\n",
    "        # If we are here then it should not be a suburl and we would just scrape directly\n",
    "        logger.info('Returning the original URL as only suburl')\n",
    "        links = [base_url]\n",
    "    \n",
    "    if len(links) == 0:\n",
    "        logger.warning('Have found {} links from {}'.format(len(links), domain))\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_headlines(suburl_html, sub_url, domain, logger):\n",
    "    \"\"\"\n",
    "    Function that will extract headlines from the HTML previously downloaded for suburls\n",
    "    \"\"\"\n",
    "    \n",
    "    # Now do source specific stuff\n",
    "    if domain == 'bbc':\n",
    "        modifier = 'football/teams' in sub_url\n",
    "        articles_info = bbc.extract_headlines(suburl_html, modifier, logger)\n",
    "\n",
    "    elif domain == 'dailymail':\n",
    "        modifier = 'football/index' not in sub_url\n",
    "        articles_info = dailymail.extract_headlines(suburl_html, modifier, logger)\n",
    "        \n",
    "    elif domain == 'theguardian':\n",
    "        articles_info = guardian.extract_headlines(suburl_html, logger)\n",
    "        \n",
    "    elif domain == 'mirror':\n",
    "        articles_info = mirror.extract_headlines(suburl_html, logger)\n",
    "    \n",
    "    elif domain == 'skysports':\n",
    "        modifier = 'regional' in sub_url\n",
    "        articles_info = skysports.extract_headlines(suburl_html, modifier, logger)\n",
    "        \n",
    "    elif domain == 'telegraph':\n",
    "        articles_info = telegraph.extract_headlines(suburl_html, logger)\n",
    "\n",
    "    return articles_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_story(story_html, domain, logger):\n",
    "    \"\"\"\n",
    "    Function to take the HTML and then get the story text according to the domain\n",
    "    \"\"\"\n",
    "\n",
    "    if domain == 'bbc':\n",
    "        story_details = bbc.get_text(story_html, logger)\n",
    "\n",
    "    elif domain == 'dailymail':\n",
    "        story_details = dailymail.get_text(story_html, logger)\n",
    "\n",
    "    elif domain == 'theguardian':\n",
    "        story_details = guardian.get_text(story_html, logger)\n",
    "\n",
    "    elif domain == 'mirror':\n",
    "        story_details = mirror.get_text(story_html, logger)\n",
    "\n",
    "    elif domain == 'skysports':\n",
    "        story_details = skysports.get_text(story_html, logger)\n",
    "\n",
    "    elif domain == 'telegraph':\n",
    "        story_details = telegraph.get_text(story_html, logger)\n",
    "\n",
    "    story_dic = {\n",
    "        'story_text' : re.sub('([a-z0-9])([A-Z])', r'\\1.\\2', story_details[0]),\n",
    "        'story_author' : story_details[1],\n",
    "        'story_date' : story_details[2],\n",
    "        'story_twitter' : story_details[3],\n",
    "        'story_keywords' : story_details[4],\n",
    "    }\n",
    "\n",
    "    return story_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(article_link, past_dates):\n",
    "    \"\"\"\n",
    "    Function for checking for duplicates and stopping us processing the same stories over and over\n",
    "    Due to the way we are saving - this can only really be done for the headlines, but\n",
    "    we can do it easily by checking those previous files with similar file names and then loading them in\n",
    "    if the URL is exactly the same, then we won't pull it\n",
    "    \n",
    "    This is important to do BEFORE we move to scrape the HTML, as this is the process that really takes longest\n",
    "    \n",
    "    1. Get the file extension\n",
    "    2. Check for the same extension else where\n",
    "    3. Since the file name is purely from the URL, we exit saying they are the same\n",
    "    \n",
    "    I think this is OK to do, as scraping is really what takes time, and we really do have to scrape the suburl HTML\n",
    "    \"\"\"\n",
    "    # Start building the file name out of the link, checking that everything is valid         \n",
    "    url_reg = re.search('\\/([^\\/][^www].*)', article_link)\n",
    "\n",
    "    # Sometimes get weird URLs of other websites\n",
    "    if url_reg:\n",
    "        url_extension = url_reg.group(1).replace('/', '_')\n",
    "    else:\n",
    "        url_extension = re.sub('[^A-z0-9]', '_', article_link)\n",
    "\n",
    "    json_name = re.sub('[^A-z0-9_]+', '', url_extension) + '.json'\n",
    "    \n",
    "    # Then move through the previous dates and try to find the link\n",
    "    for past_date in past_dates:\n",
    "        # THERE MIGHT BE A MUCH BETTER WAY TO CHECK THIS WITHOUT HAVING TO LOOP THROUGH\n",
    "        candidate_link = past_date / json_name\n",
    "        \n",
    "        # If it exists, return true and also tag on date just for ease\n",
    "        if candidate_link.exists():\n",
    "            return True, str(past_date/ json_name)\n",
    "    \n",
    "    return False, json_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinks left to consider:\n",
    "\n",
    "* Handling errors / not being able to grab links\n",
    "* Being frozen out when pulling HTML\n",
    "* Extra logging - especially info and debugging\n",
    "* Make sure that we are getting similar / equal results - need to check behaviour for fake links **NEED TO CHECK THAT WE PROPERLY NAME ARTICLES THAT KEEP SAME HEADLINE BUT UPDATE - these need to be reiewed in general and declared as special links**\n",
    "* Checking out duplicates\n",
    "* Make everything more Pathlib like\n",
    "* Instead of saving end result as a dictionary with pickle, maybe it would be better to save as a json, so we could also view it directly and more agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def full_process(baseurl_loc, proxy, logger, save_path, date_today):\n",
    "    \"\"\"\n",
    "    A function that will carry out the full, reduced process of getting the stories\n",
    "    \"\"\"\n",
    "    logger.info('Loading in the base URLs...')\n",
    "    baseurl_list = []\n",
    "    with baseurl_loc.open(mode = 'r') as list_file:\n",
    "        for url in list_file.readlines():\n",
    "            baseurl_list.append(url.rstrip())\n",
    "            \n",
    "    logger.info('Have found {}'.format(len(baseurl_list)))\n",
    "        \n",
    "    # We loop over the loaded in base urls\n",
    "    for base_url in baseurl_list:\n",
    "        # Get the domain for source specific stuff\n",
    "        domain = re.search('^.*www\\.(.*?)\\..*', base_url).group(1)\n",
    "        \n",
    "        # Get the previous dates for this domain, to check later - will also add today's date to stop \n",
    "        previous_dates = list((story_loc / domain).iterdir())\n",
    "        \n",
    "        # Just start a log to get things going\n",
    "        logger.info('Starting the process for {} which is from {}'.format(base_url, domain))\n",
    "        \n",
    "        # Get the HTML for that URL - first scrape\n",
    "        baseurl_html = process_url(base_url, proxy, logger)\n",
    "\n",
    "        if baseurl_html[1] != 'No error':\n",
    "            logger.warning('Have found an error in the baseline HTML')\n",
    "            pass\n",
    "        \n",
    "        # Then we will feed it into the suburl extractor\n",
    "        logger.info('Finding the suburls')\n",
    "        suburl_list = find_suburls(baseurl_html[0], base_url, domain, logger)\n",
    "        \n",
    "        # So we loop over the sub_urls that we found, pull the HTML and then pull out the headlines - second scrape\n",
    "        for sub_url in suburl_list:\n",
    "            suburl_html = process_url(sub_url, proxy, logger)\n",
    "            \n",
    "            if suburl_html[1] != 'No error':\n",
    "                logger.warning('Have found an error in the suburl HTML for {}'.format(sub_url))\n",
    "                continue\n",
    "            \n",
    "            # Now pull out the headlines - HAVE TO CHECK WHAT HAPPENS TO SPECIAL NON-SUBURL TYPE LINKS\n",
    "            suburl_headlines = extract_headlines(suburl_html[0], sub_url, domain, logger)\n",
    "            \n",
    "            # Now we should loop over the headlines and pull out the story - finally saving\n",
    "            logger.info('Now looking at the headlines')\n",
    "            for headline_id in suburl_headlines:\n",
    "                headline = suburl_headlines[headline_id]\n",
    "                \n",
    "                # Beforedoing anything - want to make sure that the headline is not a duplicate - also check own directory\n",
    "                is_duplicate, json_name = check_duplicates(headline['article_link'], previous_dates)\n",
    "                \n",
    "                story_path = save_path / domain / date_today\n",
    "                story_file = story_path / json_name\n",
    "                \n",
    "                if is_duplicate or story_file.exists():\n",
    "                    logger.warning('Have found a duplicate link for file named {}'.format(json_name))\n",
    "                    continue\n",
    "                \n",
    "                # First step for good headlines is to pull the HTML - third scrape\n",
    "                story_html = process_url(headline['article_link'], proxy, logger)\n",
    "                \n",
    "                if story_html[1] != 'No error':\n",
    "                    logger.warning('Have found an error in the story HTML for {}'.format(headline['article_link']))\n",
    "                    continue\n",
    "                \n",
    "                # Then get the text\n",
    "                story_details = get_story(story_html[0], domain, logger)\n",
    "                \n",
    "                # Add on to our headline\n",
    "                headline['story_text'] = story_details['story_text']\n",
    "                headline['story_author'] = story_details['story_author']\n",
    "                headline['story_date'] = story_details['story_date']\n",
    "                headline['story_twitter'] = story_details['story_twitter']\n",
    "                headline['story_keywords'] = story_details['story_keywords']\n",
    "                \n",
    "                # And finally create directory and save       \n",
    "                if not story_path.exists():\n",
    "                    story_path.mkdir(parents = True)\n",
    "                \n",
    "                with story_file.open(mode = 'w') as json_file:\n",
    "                    json.dump(headline, json_file, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1046f56fd2dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morgan_loc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'news_sources.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy_settings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstory_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_today\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-f6f5f066c4fd>\u001b[0m in \u001b[0;36mfull_process\u001b[0;34m(baseurl_loc, proxy, logger, save_path, date_today)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;31m# First step for good headlines is to pull the HTML - third scrape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mstory_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article_link'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstory_html\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'No error'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b0c88e904304>\u001b[0m in \u001b[0;36mprocess_url\u001b[0;34m(url, proxy, logger)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrequest_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The link found is not valid\\n {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b0c88e904304>\u001b[0m in \u001b[0;36mrequest_html\u001b[0;34m(url, proxy, logger)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Pull the HTML data from the URL using requests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# If we 404 then we have connected well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    506\u001b[0m         }\n\u001b[1;32m    507\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;31m# Resolve redirects if allowed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Shuffle things around if there's history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;31m# Resolve redirects if allowed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Shuffle things around if there's history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                     \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m                     \u001b[0;34m**\u001b[0m\u001b[0madapter_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m                 )\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 )\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Unexpected EOF'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/OpenSSL/SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1712\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_peek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "full_process(organ_loc / 'news_sources.txt', proxy_settings, dev_logger, story_loc, date_today)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
