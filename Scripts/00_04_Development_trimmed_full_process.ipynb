{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development\n",
    "\n",
    "This script is for development of other functions - just for simplicity of execution etc. Later the code should be moved to *football_functions*, and then deleted from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 0: Initial packages and definitions\n",
    "\n",
    "Just a block to define some stuff that we will probably never be changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base packages for running the script\n",
    "import sys, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the path and proxy in accordance with our OS\n",
    "if sys.platform == 'linux':\n",
    "    HOME_PATH = Path('/home/andreas/Desktop/Projects/Football/')\n",
    "    proxy_settings = None\n",
    "else:\n",
    "    HOME_PATH = Path('c:/Users/amathewl/Desktop/3_Personal_projects/football/')\n",
    "    proxy_settings = None\n",
    "    \n",
    "# Relative paths\n",
    "data_loc = HOME_PATH / 'Data'\n",
    "html_loc = data_loc / '01_HTML'\n",
    "organ_loc = data_loc / '00_Organisation'\n",
    "story_loc = data_loc / '02_Stories'\n",
    "\n",
    "# Get today's date for various functions\n",
    "date_today = datetime.datetime.today().strftime('%Y_%m_%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a logger for development\n",
    "from football_functions.generic import default_logger\n",
    "\n",
    "dev_logger = default_logger.get_logger(data_loc, date_today, 'development')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to do the full process, but all in one go\n",
    "\n",
    "Goal is to trim down the full process and do everything in \"one go\", only saving the final pickle and no HTML. This would be something we would use on a server online where space is more important. The process would be:\n",
    "\n",
    "* Load in the base URLs\n",
    "\n",
    "* Loop over each URL - starting with pulling the HTML\n",
    "\n",
    "* Then with the HTML in memory, look for sublinks\n",
    "\n",
    "* Then we loop over the sublinks (or the original HTML if it is \"not\" a sublink type URL)\n",
    "\n",
    "* Pull the headlines from the sublinks - loop over these and get the story\n",
    "\n",
    "* Finally save the headline and everything into a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "\n",
    "Need a function to do each step as well as pull them all together.\n",
    "\n",
    "I think want to have the initial file load in our grand function - and then scrape HTML elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time, random, requests\n",
    "\n",
    "def request_html(url, proxy, logger):\n",
    "    \"\"\"\n",
    "    Function that is fed a URL and then tries to pull from it, doing some error checking and handling proxies + sleeping\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pull the HTML data from the URL using requests\n",
    "    try:\n",
    "        response = requests.get(url, proxies = proxy)\n",
    "        \n",
    "        # If we 404 then we have connected well\n",
    "        if response.status_code == 404:\n",
    "            logger.error('404 Error')\n",
    "            return (url, '404 error')\n",
    "        else:\n",
    "            html_text = response.text\n",
    "            logger.debug('Successfully pulled HTML from {}'.format(url))\n",
    "            return (html_text, 'No error')\n",
    "    except requests.exceptions.RequestException as error:\n",
    "        # The link timed out\n",
    "        logger.warning('Have not managed to pull from {} due to a {}'.format(url, error))\n",
    "        logger.debug('Sleeping for 5 seconds to see if works again')\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Try again after sleeping for 5 seconds in case we were rejected\n",
    "        try:\n",
    "            response = requests.get(url, proxies = proxy)\n",
    "            \n",
    "            if response.status_code == 404:\n",
    "                logger.error('404 Error')\n",
    "                return (url, '404 error')\n",
    "            else:\n",
    "                html_text = response.text\n",
    "                logger.debug('Successfully pulled HTML from {}'.format(url))\n",
    "                return (html_text, 'No error')\n",
    "        except requests.exceptions.RequestException as error:\n",
    "            logger.error('Still not working')\n",
    "            return (url, error)\n",
    "\n",
    "def process_url(url, proxy, logger):\n",
    "    \"\"\"\n",
    "    A function that is given a URL and then checks it over to see if worth scraping\n",
    "    If valid, will proceed to scrape - so all scraping goes here\n",
    "    Note that if we produce an error, will return the URL again\n",
    "    \"\"\"\n",
    "    logger.info('Scraping URL {}'.format(url))\n",
    "    \n",
    "    # First check if the think we are scraping is valid and remove any spaces\n",
    "    valid_url = re.match('^http\\S*www\\.\\S*$', url)\n",
    "    \n",
    "    if valid_url and 'fake_link' not in url:\n",
    "        # Always sleep a bit before requesting, just to stop us being rejected so much\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "        \n",
    "        return request_html(valid_url.group(0).rstrip(), proxy, logger)\n",
    "    else:\n",
    "        logger.warning('The link found is not valid\\n {}\\n'.format(url))\n",
    "        return (url, 'Invalid URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import football_functions.reduced.source_specific.bbc.process_html as bbc\n",
    "import football_functions.reduced.source_specific.dailymail.process_html as dailymail\n",
    "import football_functions.reduced.source_specific.mirror.process_html as mirror\n",
    "import football_functions.reduced.source_specific.guardian.process_html as guardian\n",
    "import football_functions.reduced.source_specific.skysports.process_html as skysports\n",
    "import football_functions.reduced.source_specific.telegraph.process_html as telegraph\n",
    "\n",
    "def find_suburls(baseurl_html, base_url, domain, logger):\n",
    "    \"\"\"\n",
    "    Function that will return to us all of the suburls on the page we just downloaded\n",
    "    \"\"\"\n",
    "    links = []\n",
    "\n",
    "    # Will only process certain URLsfor each domain\n",
    "    if domain == 'bbc' and 'teams' in base_url:\n",
    "        links.extend(bbc.get_suburls(baseurl_html, logger))\n",
    "\n",
    "    elif domain == 'dailymail' and 'sport_football' in base_url:\n",
    "        links.extend(dailymail.get_suburls(baseurl_html, logger))\n",
    "\n",
    "    elif domain == 'mirror'  and 'sport_football' in base_url:\n",
    "        links.extend(mirror.get_suburls(baseurl_html, logger))\n",
    "\n",
    "    elif domain == 'theguardian'  and 'teams' in base_url:\n",
    "        links.extend(guardian.get_suburls(baseurl_html, logger))\n",
    "    else:\n",
    "        # If we are here then it should not be a suburl and we would just scrape directly\n",
    "        logger.info('Returning the original URL as only suburl')\n",
    "        links = [base_url]\n",
    "    \n",
    "    if len(links) == 0:\n",
    "        logger.warning('Have found {} links from {}'.format(len(links), domain))\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_headlines(suburl_html, sub_url, domain, logger):\n",
    "    \"\"\"\n",
    "    Function that will extract headlines from the HTML previously downloaded for suburls\n",
    "    \"\"\"\n",
    "\n",
    "    # Now do source specific stuff\n",
    "    if domain == 'bbc':\n",
    "        modifier = 'football_teams' in sub_url\n",
    "        articles_info = bbc.extract_headlines(suburl_html, modifier, logger)\n",
    "\n",
    "    elif domain == 'dailymail':\n",
    "        modifier = 'football_index' not in sub_url\n",
    "        articles_info = dailymail.extract_headlines(suburl_html, modifier, logger)\n",
    "        \n",
    "    elif domain == 'theguardian':\n",
    "        articles_info = guardian.extract_headlines(suburl_html, logger)\n",
    "        \n",
    "    elif domain == 'mirror':\n",
    "        articles_info = mirror.extract_headlines(suburl_html, logger)\n",
    "    \n",
    "    elif domain == 'skysports':\n",
    "        modifier = 'regional' in sub_url\n",
    "        articles_info = skysports.extract_headlines(suburl_html, modifier, logger)\n",
    "        \n",
    "    elif domain == 'telegraph':\n",
    "        articles_info = telegraph.extract_headlines(suburl_html, logger)\n",
    "\n",
    "    return articles_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_story(story_html, domain, logger):\n",
    "    \"\"\"\n",
    "    Function to take the HTML and then get the story text according to the domain\n",
    "    \"\"\"\n",
    "\n",
    "    if domain == 'bbc':\n",
    "        story_details = bbc.get_text(story_html, logger)\n",
    "\n",
    "    elif domain == 'dailymail':\n",
    "        story_details = dailymail.get_text(story_html, logger)\n",
    "\n",
    "    elif domain == 'theguardian':\n",
    "        story_details = guardian.get_text(story_html, logger)\n",
    "\n",
    "    elif domain == 'mirror':\n",
    "        story_details = mirror.get_text(story_html, logger)\n",
    "\n",
    "    elif domain == 'skysports':\n",
    "        story_details = skysports.get_text(story_html, logger)\n",
    "\n",
    "    elif domain == 'telegraph':\n",
    "        story_details = telegraph.get_text(story_html, logger)\n",
    "\n",
    "    story_dic = {\n",
    "        'story_text' : re.sub('([a-z0-9])([A-Z])', r'\\1.\\2', story_details[0]),\n",
    "        'story_author' : story_details[1],\n",
    "        'story_date' : story_details[2],\n",
    "        'story_twitter' : story_details[3],\n",
    "        'story_keywords' : story_details[4],\n",
    "    }\n",
    "\n",
    "    return story_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(article_link, past_dates):\n",
    "    \"\"\"\n",
    "    Function for checking for duplicates and stopping us processing the same stories over and over\n",
    "    Due to the way we are saving - this can only really be done for the headlines, but\n",
    "    we can do it easily by checking those previous files with similar file names and then loading them in\n",
    "    if the URL is exactly the same, then we won't pull it\n",
    "    \n",
    "    This is important to do BEFORE we move to scrape the HTML, as this is the process that really takes longest\n",
    "    \n",
    "    1. Get the file extension\n",
    "    2. Check for the same extension else where\n",
    "    3. Since the file name is purely from the URL, we exit saying they are the same\n",
    "    \n",
    "    I think this is OK to do, as scraping is really what takes time, and we really do have to scrape the suburl HTML\n",
    "    \"\"\"\n",
    "    # Start building the file name out of the link, checking that everything is valid         \n",
    "    url_reg = re.search('\\/([^\\/][^www].*)', article_link)\n",
    "\n",
    "    # Sometimes get weird URLs of other websites\n",
    "    if url_reg:\n",
    "        url_extension = url_reg.group(1).replace('/', '_')\n",
    "    else:\n",
    "        url_extension = re.sub('[^A-z0-9]', '_', article_link)\n",
    "\n",
    "    json_name = re.sub('[^A-z0-9_]+', '', url_extension) + '.json'\n",
    "    \n",
    "    # Then move through the previous dates and try to find the link\n",
    "    for past_date in past_dates:\n",
    "        # THERE MIGHT BE A MUCH BETTER WAY TO CHECK THIS WITHOUT HAVING TO LOOP THROUGH\n",
    "        candidate_link = past_date / json_name\n",
    "        \n",
    "        # If it exists, return true and also tag on date just for ease\n",
    "        if candidate_link.exists():\n",
    "            return True, str(past_date/ json_name)\n",
    "    \n",
    "    return False, json_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinks left to consider:\n",
    "\n",
    "* Handling errors / not being able to grab links\n",
    "* Being frozen out when pulling HTML\n",
    "* Extra logging - especially info and debugging\n",
    "* Make sure that we are getting similar / equal results - need to check behaviour for fake links **NEED TO CHECK THAT WE PROPERLY NAME ARTICLES THAT KEEP SAME HEADLINE BUT UPDATE - these need to be reiewed in general and declared as special links**\n",
    "* Checking out duplicates\n",
    "* Make everything more Pathlib like\n",
    "* Instead of saving end result as a dictionary with pickle, maybe it would be better to save as a json, so we could also view it directly and more agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def full_process(baseurl_loc, proxy, logger, save_path, date_today):\n",
    "    \"\"\"\n",
    "    A function that will carry out the full, reduced process of getting the stories\n",
    "    \"\"\"\n",
    "    logger.info('Loading in the base URLs...')\n",
    "    baseurl_list = []\n",
    "    with baseurl_loc.open(mode = 'r') as list_file:\n",
    "        for url in list_file.readlines():\n",
    "            baseurl_list.append(url.rstrip())\n",
    "            \n",
    "    logger.info('Have found {}'.format(len(baseurl_list)))\n",
    "        \n",
    "    # We loop over the loaded in base urls\n",
    "    for base_url in baseurl_list:\n",
    "        # Get the domain for source specific stuff\n",
    "        domain = re.search('^.*www\\.(.*?)\\..*', base_url).group(1)\n",
    "        \n",
    "        # Get the previous dates for this domain, to check later\n",
    "        previous_dates = list((story_loc / domain).iterdir())\n",
    "        \n",
    "        # Just start a log to get things going\n",
    "        logger.info('Starting the process for {} which is from {}'.format(base_url, domain))\n",
    "        \n",
    "        # Get the HTML for that URL - first scrape\n",
    "        baseurl_html = process_url(base_url, proxy, logger)\n",
    "\n",
    "        if baseurl_html[1] != 'No error':\n",
    "            logger.warning('Have found an error in the baseline HTML')\n",
    "            pass\n",
    "        \n",
    "        # Then we will feed it into the suburl extractor\n",
    "        logger.info('Finding the suburls')\n",
    "        suburl_list = find_suburls(baseurl_html[0], base_url, domain, logger)\n",
    "        \n",
    "        # So we loop over the sub_urls that we found, pull the HTML and then pull out the headlines - second scrape\n",
    "        for sub_url in suburl_list:\n",
    "            suburl_html = process_url(sub_url, proxy, logger)\n",
    "            \n",
    "            if suburl_html[1] != 'No error':\n",
    "                logger.warning('Have found an error in the suburl HTML for {}'.format(sub_url))\n",
    "                pass\n",
    "            \n",
    "            # Now pull out the headlines - HAVE TO CHECK WHAT HAPPENS TO SPECIAL NON-SUBURL TYPE LINKS\n",
    "            suburl_headlines = extract_headlines(suburl_html[0], sub_url, domain, logger)\n",
    "            \n",
    "            # Now we should loop over the headlines and pull out the story - finally saving\n",
    "            logger.info('Now looking at the headlines')\n",
    "            for headline_id in suburl_headlines:\n",
    "                headline = suburl_headlines[headline_id]\n",
    "                \n",
    "                # Beforedoing anything - want to make sure that the headline is not a duplicate\n",
    "                is_duplicate, json_name = check_duplicates(headline['article_link'], previous_dates)\n",
    "                \n",
    "                if is_duplicate:\n",
    "                    logger.warning('Have found a duplicate link for file named {}'.format(json_name))\n",
    "                    pass\n",
    "                \n",
    "                # First step for good headlines is to pull the HTML - third scrape\n",
    "                story_html = process_url(headline['article_link'], proxy, logger)\n",
    "                \n",
    "                if story_html[1] != 'No error':\n",
    "                    logger.warning('Have found an error in the story HTML for {}'.format(headline['article_link']))\n",
    "                    pass\n",
    "                \n",
    "                # Then get the text\n",
    "                story_details = get_story(story_html[0], domain, logger)\n",
    "                \n",
    "                # Add on to our headline\n",
    "                headline['story_text'] = story_details['story_text']\n",
    "                headline['story_author'] = story_details['story_author']\n",
    "                headline['story_date'] = story_details['story_date']\n",
    "                headline['story_twitter'] = story_details['story_twitter']\n",
    "                headline['story_keywords'] = story_details['story_keywords']\n",
    "                \n",
    "                # And finally save\n",
    "                story_path = save_path / domain / date_today\n",
    "                \n",
    "                if not story_path.exists():\n",
    "                    story_path.mkdir(parents = True)\n",
    "                \n",
    "                story_file = story_path / json_name\n",
    "                \n",
    "                with story_file.open(mode = 'w') as json_file:\n",
    "                    json.dump(headline, json_file, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_process(organ_loc / 'news_sources.txt', proxy_settings, dev_logger, story_loc, date_today)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
