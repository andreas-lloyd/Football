Status:
    - The full process seems complete, with a de-duplication part included
    - Nothing is parallelised or anything like that, but I think that is a long way off
Next tasks:
- Script to move files into cloud
    - I guess it should look at which files are already there and then move those that are not
    - Do every few days I guess
- Script to summarise logs

Immediate to do:
    - Check that de-duplication is working properly - especially for links that may not change name but update internally
        - Need to first identify the stories that do not change name, as I am not clear
        - Need to make some system that can catch this automatically
        - One way to get around duplicates would be to insert fake link and just not check for those > but then we will eventually start saving duplicates (that could be removed later I guess)

    - Consider also getting the title from the story > when scraping gossip, we get titles that are just the names, but later the actual title is quite different

    - Something that looks at the logs and summarises the progress
        - number of errors + printing uniques and counts + percentage of domain
        - number of duplicates + percentage of domain

    - ISSUE:
        - Have found that some team pages are not scraping because the domain name isn't saving (e.g. url = /sport/teams/barcelona)

    URGENT:
        - String operation on posix path
        - NEED to do something to make immediately obvious that there is a big problem - i.e. error on exit  >>> ADD DATE TO IT
        >> KEYBOARD INTERRUPT NOT WORKING

    I'm not sure if saving directly to buckets is worth it, because it is taking an unreasonably long time to do anything
        Don't know if this is due to anything specific, or just really slow working over internet

    Minor change:
        - Adjust level of somet stuff, loads of errors marked as errors

Medium term improvements:
- Improve print statements to get some feedback on what is going on as running
- Script that analyses the log and reports progress
- Bash script for general install (including update python, pip3, parsel, chromjob and buckets)
- Make more package like ? not sure if needed if just want to pull in the github - i guess could make the function part more like a function

Data processing to do:
- Need to start extracting key terms
- Then set up pipeline for identifying which stories are the same - filtering by terms

Future to do:
    - Re-think how feeding in baseurls, maybe there is a more flexible way and would be better for long term stability
    - Options for saving to a data base and what would be the best path
    - Once decided - start process of processing text data for injest - maybe should start injest immediately with the raw stories so that processing works from that
    - Once completed - join the process of scraping and data processing + do look back processing
    - Injest into database
    - Parallelise tasks (in a basic way)

Server notes:
    - Would be nice to set up the SSH to work with private / public keys so that don't need to do much else
    - Need to figure out if the database should be in a different place to where the core process runs

2018-11-18 Report:
- Memory saving worked really well - have saved only 20MB for more or less 2 days
- Seems like de-duplication is working OK > still need to iron out those links that should be duplicated (probably by altering name)
- 

2018-12-08 Report:
- Progressing OK - downloaded all logs

2019-01-27
- Considering stopping saving directly to buckets and doing a CP over to buckets every month or something, as it looks to be progressing extremely slowly
- I think should try and come back to this at a later point when sure that the process is solid
- Could make a process to tar the jsons and copy over every now and again
- Could also save the log report there
- CLEARLY see that logging doesn't work well, for example > maybe just generally more stable to do everything locally, and then later compress everything and send over
- This could be done in the same script just executing shell commands to tar and copy (cannot move or would pull duplicates)

TO DO:
- Analyse logs for:
    - Repeated deletions
    - Errors in scraping (from issue)